{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP89BaAXAoI0RR7+VzPAeTn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Crawling Data\n","Data crawling atau perayapan data adalah proses pengambilan data yang tersedia secara online untuk umum. Proses ini kemudian mengimpor informasi atau data yang telah ditemukan ke dalam file lokal di komputer Anda."],"metadata":{"id":"Ny91ItIEukzp"}},{"cell_type":"markdown","source":["# Scrapy\n","Library scrapy tidak hanya digunakan untuk web scraping saja, tetapi juga keseluruhan framework untuk mendukung kebutuhan web scraping pada satu platform. Scrapy dikembangkan oleh salah satu pendiri Scraping hub, Shane Evans dan Pablo Hoffman. Library ini digunakan untuk project web scraping besar. Keuntungan menggunakan library ini adalah library ini bersifat asynchronous, dokumentasi yang lebih baik, bisa menggunakan berbagai plugin, bisa dikombinasikan dengan pipeline dan middlewares buatan, hemat memori dan dapat digunakan dengan CPU rendah. Namun, walaupun memiliki berbagai kelebihan, library ini tetap memiliki kelemahan, yaitu kurang ramah bagi pemula yang benar-benar baru belajar web scraping."],"metadata":{"id":"jfg14kJYuqM0"}},{"cell_type":"markdown","source":["# Install Scrapy"],"metadata":{"id":"ckvjsb2swCDU"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LdTa_mL6sfq8","executionInfo":{"status":"ok","timestamp":1667314693817,"user_tz":-420,"elapsed":16339,"user":{"displayName":"Makhdum Uhuy","userId":"01560497234901229144"}},"outputId":"01425b13-35a8-4a95-c9f1-eac3a97d8280"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting scrapy\n","  Downloading Scrapy-2.7.0-py2.py3-none-any.whl (270 kB)\n","\u001b[K     |████████████████████████████████| 270 kB 6.8 MB/s \n","\u001b[?25hCollecting itemloaders>=1.0.1\n","  Downloading itemloaders-1.0.6-py3-none-any.whl (11 kB)\n","Collecting Twisted>=18.9.0\n","  Downloading Twisted-22.10.0-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 53.2 MB/s \n","\u001b[?25hCollecting zope.interface>=5.1.0\n","  Downloading zope.interface-5.5.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (254 kB)\n","\u001b[K     |████████████████████████████████| 254 kB 70.6 MB/s \n","\u001b[?25hCollecting protego>=0.1.15\n","  Downloading Protego-0.2.1-py2.py3-none-any.whl (8.2 kB)\n","Collecting itemadapter>=0.1.0\n","  Downloading itemadapter-0.7.0-py3-none-any.whl (10 kB)\n","Collecting service-identity>=18.1.0\n","  Downloading service_identity-21.1.0-py2.py3-none-any.whl (12 kB)\n","Collecting cryptography>=3.3\n","  Downloading cryptography-38.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n","\u001b[K     |████████████████████████████████| 4.0 MB 44.1 MB/s \n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from scrapy) (57.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from scrapy) (21.3)\n","Collecting tldextract\n","  Downloading tldextract-3.4.0-py3-none-any.whl (93 kB)\n","\u001b[K     |████████████████████████████████| 93 kB 3.2 MB/s \n","\u001b[?25hRequirement already satisfied: lxml>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (4.9.1)\n","Collecting queuelib>=1.4.2\n","  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n","Collecting cssselect>=0.9.1\n","  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n","Collecting pyOpenSSL>=21.0.0\n","  Downloading pyOpenSSL-22.1.0-py3-none-any.whl (57 kB)\n","\u001b[K     |████████████████████████████████| 57 kB 6.1 MB/s \n","\u001b[?25hCollecting w3lib>=1.17.0\n","  Downloading w3lib-2.0.1-py3-none-any.whl (20 kB)\n","Collecting parsel>=1.5.0\n","  Downloading parsel-1.7.0-py2.py3-none-any.whl (14 kB)\n","Collecting PyDispatcher>=2.0.5\n","  Downloading PyDispatcher-2.0.6.tar.gz (38 kB)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=3.3->scrapy) (1.15.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=3.3->scrapy) (2.21)\n","Collecting jmespath>=0.9.5\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from protego>=0.1.15->scrapy) (1.15.0)\n","Requirement already satisfied: pyasn1 in /usr/local/lib/python3.7/dist-packages (from service-identity>=18.1.0->scrapy) (0.4.8)\n","Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.7/dist-packages (from service-identity>=18.1.0->scrapy) (0.2.8)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from service-identity>=18.1.0->scrapy) (22.1.0)\n","Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from Twisted>=18.9.0->scrapy) (4.1.1)\n","Collecting hyperlink>=17.1.1\n","  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n","\u001b[K     |████████████████████████████████| 74 kB 3.7 MB/s \n","\u001b[?25hCollecting Automat>=0.8.0\n","  Downloading Automat-22.10.0-py2.py3-none-any.whl (26 kB)\n","Collecting incremental>=21.3.0\n","  Downloading incremental-22.10.0-py2.py3-none-any.whl (16 kB)\n","Collecting constantly>=15.1\n","  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n","Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.7/dist-packages (from hyperlink>=17.1.1->Twisted>=18.9.0->scrapy) (2.10)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->scrapy) (3.0.9)\n","Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract->scrapy) (3.8.0)\n","Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tldextract->scrapy) (2.23.0)\n","Collecting requests-file>=1.4\n","  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->scrapy) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->scrapy) (1.24.3)\n","Building wheels for collected packages: PyDispatcher\n","  Building wheel for PyDispatcher (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.6-py3-none-any.whl size=11958 sha256=6e2b44b67a496ff74c8cbe8173c97b4a07b2b700f12686c95530d14d299e6096\n","  Stored in directory: /root/.cache/pip/wheels/c9/d6/6a/de198d890277cde60ca3dbebe7ae592d3b381c7d9bb2455f4d\n","Successfully built PyDispatcher\n","Installing collected packages: w3lib, cssselect, zope.interface, requests-file, parsel, jmespath, itemadapter, incremental, hyperlink, cryptography, constantly, Automat, Twisted, tldextract, service-identity, queuelib, pyOpenSSL, PyDispatcher, protego, itemloaders, scrapy\n","Successfully installed Automat-22.10.0 PyDispatcher-2.0.6 Twisted-22.10.0 constantly-15.1.0 cryptography-38.0.1 cssselect-1.2.0 hyperlink-21.0.0 incremental-22.10.0 itemadapter-0.7.0 itemloaders-1.0.6 jmespath-1.0.1 parsel-1.7.0 protego-0.2.1 pyOpenSSL-22.1.0 queuelib-1.6.2 requests-file-1.5.1 scrapy-2.7.0 service-identity-21.1.0 tldextract-3.4.0 w3lib-2.0.1 zope.interface-5.5.0\n"]}],"source":["pip install scrapy"]},{"cell_type":"markdown","source":["# Membuat File Project Scrapy"],"metadata":{"id":"lrMn6xdhwFY3"}},{"cell_type":"code","source":["!scrapy startproject scrapyPTA"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jWwQtY4wtNZL","executionInfo":{"status":"ok","timestamp":1667315008197,"user_tz":-420,"elapsed":1647,"user":{"displayName":"Makhdum Uhuy","userId":"01560497234901229144"}},"outputId":"5d621102-3118-4da7-9acb-83508dcb9032"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["New Scrapy project 'scrapyPTA', using template directory '/usr/local/lib/python3.7/dist-packages/scrapy/templates/project', created in:\n","    /content/scrapyPTA\n","\n","You can start your first spider with:\n","    cd scrapyPTA\n","    scrapy genspider example example.com\n"]}]},{"cell_type":"markdown","source":["# Masuk Kedalam Folder Scrapy yang Sudah Kita Buat"],"metadata":{"id":"3mLEruaxwO83"}},{"cell_type":"code","source":["%cd scrapyPTA/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PRoIit9YtRl5","executionInfo":{"status":"ok","timestamp":1667315016326,"user_tz":-420,"elapsed":487,"user":{"displayName":"Makhdum Uhuy","userId":"01560497234901229144"}},"outputId":"d04d7f8a-348e-4ed9-f161-52e47cd7e7b2"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/scrapyPTA\n"]}]},{"cell_type":"markdown","source":["# Membuat File Spider"],"metadata":{"id":"IA1-OBtIwcDA"}},{"cell_type":"code","source":["!scrapy genspider webpta pta.trunojoyo.ac.id"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"19UHvI9ptSHg","executionInfo":{"status":"ok","timestamp":1667315078008,"user_tz":-420,"elapsed":2330,"user":{"displayName":"Makhdum Uhuy","userId":"01560497234901229144"}},"outputId":"6012e624-44c1-4bf1-c7f1-7d0e5fb252be"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Created spider 'webpta' using template 'basic' in module:\n","  scrapyPTA.spiders.webpta\n"]}]},{"cell_type":"markdown","source":["# Import Scrapy"],"metadata":{"id":"xCnBMVSTwdZJ"}},{"cell_type":"code","source":["import scrapy"],"metadata":{"id":"u-C_OtEwsq2T","executionInfo":{"status":"ok","timestamp":1667315082322,"user_tz":-420,"elapsed":1358,"user":{"displayName":"Makhdum Uhuy","userId":"01560497234901229144"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["# Proses Crawling Data PTA"],"metadata":{"id":"RwL1J3Ejwgs9"}},{"cell_type":"code","source":["class scrapPTA(scrapy.Spider):\n","    name = 'PTA'\n","    allowed_domains = ['pta.trunojoyo.ac.id']\n","    start_urls = ['https://pta.trunojoyo.ac.id/c_search/byprod/5/' +\n","                  str(x)+\" \" for x in range(1, 10)]\n","\n","    def parse(self, response):\n","        for link in response.css('a.gray.button::attr(href)'):\n","            yield response.follow(link.get(), callback=self.parse_categories)\n","\n","    def parse_categories(self, response):\n","        products = response.css('div#content_journal ul li')\n","        for product in products:\n","            yield {\n","                'abstrak': product.css('div div:nth-child(2) p::text').get().strip()\n","            }"],"metadata":{"id":"UxYDiQUZstU6","executionInfo":{"status":"ok","timestamp":1667315084753,"user_tz":-420,"elapsed":4,"user":{"displayName":"Makhdum Uhuy","userId":"01560497234901229144"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# Masuk Kedalam Folder Spiders"],"metadata":{"id":"sGwpQPxSwmrq"}},{"cell_type":"code","source":["%cd scrapyPTA/spiders/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O8AAnm2Ns5cG","executionInfo":{"status":"ok","timestamp":1667315087420,"user_tz":-420,"elapsed":5,"user":{"displayName":"Makhdum Uhuy","userId":"01560497234901229144"}},"outputId":"9552fc63-5f02-4ab0-ef53-edf4e3021e25"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/scrapyPTA/scrapyPTA/spiders\n"]}]},{"cell_type":"code","source":["!scrapy runspider webpta.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RYTIU4Fbt5Hq","executionInfo":{"status":"ok","timestamp":1667315094254,"user_tz":-420,"elapsed":4454,"user":{"displayName":"Makhdum Uhuy","userId":"01560497234901229144"}},"outputId":"b98402e1-34cc-4e24-90cc-989e6efe8f1d"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["2022-11-01 15:04:50 [scrapy.utils.log] INFO: Scrapy 2.7.0 started (bot: scrapyPTA)\n","2022-11-01 15:04:50 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.7.0, w3lib 2.0.1, Twisted 22.10.0, Python 3.7.15 (default, Oct 12 2022, 19:14:55) - [GCC 7.5.0], pyOpenSSL 22.1.0 (OpenSSL 3.0.5 5 Jul 2022), cryptography 38.0.1, Platform Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic\n","2022-11-01 15:04:50 [scrapy.crawler] INFO: Overridden settings:\n","{'BOT_NAME': 'scrapyPTA',\n"," 'NEWSPIDER_MODULE': 'scrapyPTA.spiders',\n"," 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n"," 'ROBOTSTXT_OBEY': True,\n"," 'SPIDER_LOADER_WARN_ONLY': True,\n"," 'SPIDER_MODULES': ['scrapyPTA.spiders'],\n"," 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n","2022-11-01 15:04:50 [asyncio] DEBUG: Using selector: EpollSelector\n","2022-11-01 15:04:50 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n","2022-11-01 15:04:50 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n","2022-11-01 15:04:50 [scrapy.extensions.telnet] INFO: Telnet Password: ed02e359270c571a\n","2022-11-01 15:04:50 [scrapy.middleware] INFO: Enabled extensions:\n","['scrapy.extensions.corestats.CoreStats',\n"," 'scrapy.extensions.telnet.TelnetConsole',\n"," 'scrapy.extensions.memusage.MemoryUsage',\n"," 'scrapy.extensions.logstats.LogStats']\n","2022-11-01 15:04:50 [scrapy.middleware] INFO: Enabled downloader middlewares:\n","['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n"," 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n"," 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n"," 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n"," 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n"," 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n"," 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n"," 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n"," 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n"," 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n","2022-11-01 15:04:50 [scrapy.middleware] INFO: Enabled spider middlewares:\n","['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n"," 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n"," 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n"," 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n"," 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n","2022-11-01 15:04:50 [scrapy.middleware] INFO: Enabled item pipelines:\n","[]\n","2022-11-01 15:04:50 [scrapy.core.engine] INFO: Spider opened\n","2022-11-01 15:04:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n","2022-11-01 15:04:50 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n","2022-11-01 15:04:50 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://pta.trunojoyo.ac.id/robots.txt> from <GET http://pta.trunojoyo.ac.id/robots.txt>\n","/usr/local/lib/python3.7/dist-packages/scrapy/core/engine.py:279: ScrapyDeprecationWarning: Passing a 'spider' argument to ExecutionEngine.download is deprecated\n","  return self.download(result, spider) if isinstance(result, Request) else result\n","2022-11-01 15:04:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://pta.trunojoyo.ac.id/robots.txt> (referer: None)\n","2022-11-01 15:04:52 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://pta.trunojoyo.ac.id/> from <GET http://pta.trunojoyo.ac.id/>\n","2022-11-01 15:04:52 [filelock] DEBUG: Attempting to acquire lock 140712853657744 on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n","2022-11-01 15:04:52 [filelock] DEBUG: Lock 140712853657744 acquired on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n","2022-11-01 15:04:52 [filelock] DEBUG: Attempting to acquire lock 140712853705168 on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/urls/62bf135d1c2f3d4db4228b9ecaf507a2.tldextract.json.lock\n","2022-11-01 15:04:52 [filelock] DEBUG: Lock 140712853705168 acquired on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/urls/62bf135d1c2f3d4db4228b9ecaf507a2.tldextract.json.lock\n","2022-11-01 15:04:52 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): publicsuffix.org:443\n","2022-11-01 15:04:53 [urllib3.connectionpool] DEBUG: https://publicsuffix.org:443 \"GET /list/public_suffix_list.dat HTTP/1.1\" 200 None\n","2022-11-01 15:04:53 [filelock] DEBUG: Attempting to release lock 140712853705168 on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/urls/62bf135d1c2f3d4db4228b9ecaf507a2.tldextract.json.lock\n","2022-11-01 15:04:53 [filelock] DEBUG: Lock 140712853705168 released on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/urls/62bf135d1c2f3d4db4228b9ecaf507a2.tldextract.json.lock\n","2022-11-01 15:04:53 [filelock] DEBUG: Attempting to release lock 140712853657744 on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n","2022-11-01 15:04:53 [filelock] DEBUG: Lock 140712853657744 released on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n","2022-11-01 15:04:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://pta.trunojoyo.ac.id/> (referer: None)\n","2022-11-01 15:04:53 [scrapy.core.engine] INFO: Closing spider (finished)\n","2022-11-01 15:04:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n","{'downloader/request_bytes': 896,\n"," 'downloader/request_count': 4,\n"," 'downloader/request_method_count/GET': 4,\n"," 'downloader/response_bytes': 7310,\n"," 'downloader/response_count': 4,\n"," 'downloader/response_status_count/200': 2,\n"," 'downloader/response_status_count/301': 2,\n"," 'elapsed_time_seconds': 2.862145,\n"," 'finish_reason': 'finished',\n"," 'finish_time': datetime.datetime(2022, 11, 1, 15, 4, 53, 165828),\n"," 'httpcompression/response_bytes': 17488,\n"," 'httpcompression/response_count': 1,\n"," 'log_count/DEBUG': 17,\n"," 'log_count/INFO': 10,\n"," 'memusage/max': 92938240,\n"," 'memusage/startup': 92938240,\n"," 'response_received_count': 2,\n"," 'robotstxt/request_count': 1,\n"," 'robotstxt/response_count': 1,\n"," 'robotstxt/response_status_count/200': 1,\n"," 'scheduler/dequeued': 2,\n"," 'scheduler/dequeued/memory': 2,\n"," 'scheduler/enqueued': 2,\n"," 'scheduler/enqueued/memory': 2,\n"," 'start_time': datetime.datetime(2022, 11, 1, 15, 4, 50, 303683)}\n","2022-11-01 15:04:53 [scrapy.core.engine] INFO: Spider closed (finished)\n"]}]},{"cell_type":"markdown","source":["# Simpan Hasil Crawling Data PTA"],"metadata":{"id":"BaAZCDxkwz70"}},{"cell_type":"code","source":["!scrapy crawl webpta -O crawlAbstrak.csv"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1nHx8T0mt7eH","executionInfo":{"status":"ok","timestamp":1667315103049,"user_tz":-420,"elapsed":3883,"user":{"displayName":"Makhdum Uhuy","userId":"01560497234901229144"}},"outputId":"5a311ffd-c6c3-47ce-a9cd-8ea35f72105f"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["2022-11-01 15:04:59 [scrapy.utils.log] INFO: Scrapy 2.7.0 started (bot: scrapyPTA)\n","2022-11-01 15:04:59 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.7.0, w3lib 2.0.1, Twisted 22.10.0, Python 3.7.15 (default, Oct 12 2022, 19:14:55) - [GCC 7.5.0], pyOpenSSL 22.1.0 (OpenSSL 3.0.5 5 Jul 2022), cryptography 38.0.1, Platform Linux-5.10.133+-x86_64-with-Ubuntu-18.04-bionic\n","2022-11-01 15:04:59 [scrapy.crawler] INFO: Overridden settings:\n","{'BOT_NAME': 'scrapyPTA',\n"," 'NEWSPIDER_MODULE': 'scrapyPTA.spiders',\n"," 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n"," 'ROBOTSTXT_OBEY': True,\n"," 'SPIDER_MODULES': ['scrapyPTA.spiders'],\n"," 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n","2022-11-01 15:04:59 [asyncio] DEBUG: Using selector: EpollSelector\n","2022-11-01 15:04:59 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n","2022-11-01 15:04:59 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n","2022-11-01 15:04:59 [scrapy.extensions.telnet] INFO: Telnet Password: 1feef8d0ffa7f74d\n","2022-11-01 15:04:59 [scrapy.middleware] INFO: Enabled extensions:\n","['scrapy.extensions.corestats.CoreStats',\n"," 'scrapy.extensions.telnet.TelnetConsole',\n"," 'scrapy.extensions.memusage.MemoryUsage',\n"," 'scrapy.extensions.feedexport.FeedExporter',\n"," 'scrapy.extensions.logstats.LogStats']\n","2022-11-01 15:04:59 [scrapy.middleware] INFO: Enabled downloader middlewares:\n","['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n"," 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n"," 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n"," 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n"," 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n"," 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n"," 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n"," 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n"," 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n"," 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n","2022-11-01 15:04:59 [scrapy.middleware] INFO: Enabled spider middlewares:\n","['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n"," 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n"," 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n"," 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n"," 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n","2022-11-01 15:04:59 [scrapy.middleware] INFO: Enabled item pipelines:\n","[]\n","2022-11-01 15:04:59 [scrapy.core.engine] INFO: Spider opened\n","2022-11-01 15:04:59 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n","2022-11-01 15:04:59 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n","2022-11-01 15:05:00 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://pta.trunojoyo.ac.id/robots.txt> from <GET http://pta.trunojoyo.ac.id/robots.txt>\n","/usr/local/lib/python3.7/dist-packages/scrapy/core/engine.py:279: ScrapyDeprecationWarning: Passing a 'spider' argument to ExecutionEngine.download is deprecated\n","  return self.download(result, spider) if isinstance(result, Request) else result\n","2022-11-01 15:05:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://pta.trunojoyo.ac.id/robots.txt> (referer: None)\n","2022-11-01 15:05:01 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to <GET https://pta.trunojoyo.ac.id/> from <GET http://pta.trunojoyo.ac.id/>\n","2022-11-01 15:05:02 [filelock] DEBUG: Attempting to acquire lock 140057015372880 on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n","2022-11-01 15:05:02 [filelock] DEBUG: Lock 140057015372880 acquired on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n","2022-11-01 15:05:02 [filelock] DEBUG: Attempting to release lock 140057015372880 on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n","2022-11-01 15:05:02 [filelock] DEBUG: Lock 140057015372880 released on /root/.cache/python-tldextract/3.7.15.final__usr__7d8fdf__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n","2022-11-01 15:05:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://pta.trunojoyo.ac.id/> (referer: None)\n","2022-11-01 15:05:02 [scrapy.core.engine] INFO: Closing spider (finished)\n","2022-11-01 15:05:02 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n","{'downloader/request_bytes': 896,\n"," 'downloader/request_count': 4,\n"," 'downloader/request_method_count/GET': 4,\n"," 'downloader/response_bytes': 7306,\n"," 'downloader/response_count': 4,\n"," 'downloader/response_status_count/200': 2,\n"," 'downloader/response_status_count/301': 2,\n"," 'elapsed_time_seconds': 2.679217,\n"," 'finish_reason': 'finished',\n"," 'finish_time': datetime.datetime(2022, 11, 1, 15, 5, 2, 579642),\n"," 'httpcompression/response_bytes': 17488,\n"," 'httpcompression/response_count': 1,\n"," 'log_count/DEBUG': 11,\n"," 'log_count/INFO': 10,\n"," 'memusage/max': 93007872,\n"," 'memusage/startup': 93007872,\n"," 'response_received_count': 2,\n"," 'robotstxt/request_count': 1,\n"," 'robotstxt/response_count': 1,\n"," 'robotstxt/response_status_count/200': 1,\n"," 'scheduler/dequeued': 2,\n"," 'scheduler/dequeued/memory': 2,\n"," 'scheduler/enqueued': 2,\n"," 'scheduler/enqueued/memory': 2,\n"," 'start_time': datetime.datetime(2022, 11, 1, 15, 4, 59, 900425)}\n","2022-11-01 15:05:02 [scrapy.core.engine] INFO: Spider closed (finished)\n"]}]}]}